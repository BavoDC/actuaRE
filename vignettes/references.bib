@book{Ohlsson,
series = {EAA Lecture Notes},
issn = {3-642-10790-7},
abstract = {Setting the price of a non-life insurance policy involves the statistical analysis of insurance data, taking into consideration various properties of the insured object and the policy holder. Introduced by British actuaries, generalized linear models (GLMs) have by now become a standard approach used for pricing in many countries. The book focuses on methods based on GLMs that have been found useful in actuarial practice. Basic theory of GLMs in an insurance setting is presented, with useful extensions that are not in common use. The book can be used in actuarial education designed to meet the European Core Syllabus and is written for actuarial students as well as practicing actuaries. To support the readers, it contains case studies using real data of some complexity that are available on the www.},
publisher = {Springer Berlin Heidelberg : Imprint: Springer},
isbn = {1280391626},
year = {2010},
title = {Non-Life Insurance Pricing with Generalized Linear Models},
language = {eng},
address = {Berlin, Heidelberg},
keywords = {Mathematics.},
author={Ohlsson, E. and Johansson, B.},
}

@article{Ohlsson2008,
issn = {0346-1238},
abstract = {car model classification Rating of non-life insurance contracts commonly employs multiplicative models, which are estimated by generalized linear models (GLMs); another useful tool for rate making are credibility models. The object of this paper is to demonstrate how these can be combined in practice, to solve the problem with multi-level factors â€“ rating factors with too many levels for GLM estimation. In particular, we consider car model classification in motor insurance, using data from a Swedish insurance company.},
journal = {Scandinavian Actuarial Journal},
pages = {301--314},
volume = {2008},
publisher = {Taylor \& Francis Group},
number = {4},
year = {2008},
title = {Combining generalized linear models and credibility models in practice},
author = {Ohlsson, E.},
keywords = {Generalized Linear Models ; Credibility Theory ; Hierarchical Credibility ; Car Model Classification ; Motor Insurance ; Multi-Level Factor},
}

@article{Micci2001,
issn = {1931-0145},
abstract = {Categorical data fields characterized by a large number of distinct values represent a serious challenge for many classification and regression algorithms that require numerical inputs. On the other hand, these types of data fields are quite common in real-world data mining applications and often contain potentially relevant information that is difficult to represent for modeling purposes.This paper presents a simple preprocessing scheme for high-cardinality categorical data that allows this class of attributes to be used in predictive models such as neural networks, linear and logistic regression. The proposed method is based on a well-established statistical method (empirical Bayes) that is straightforward to implement as an in-database procedure. Furthermore, for categorical attributes with an inherent hierarchical structure, like ZIP codes, the preprocessing scheme can directly leverage the hierarchy by blending statistics at the various levels of aggregation.While the statistical methods discussed in this paper were first introduced in the mid 1950's, the use of these methods as a preprocessing step for complex models, like neural networks, has not been previously discussed in any literature.},
journal = {SIGKDD explorations},
pages = {27--32},
volume = {3},
publisher = {ACM},
number = {1},
year = {2001},
title = {A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems},
language = {eng},
author = {Micci-Barreca, D.},
keywords = {empirical bayes ; categorical attributes ; predictive models ; neural networks ; hierarchical attributes},
}

@article{Pargent2021,
abstract = {Because most machine learning (ML) algorithms are designed for numerical
inputs, efficiently encoding categorical variables is a crucial aspect during
data analysis. An often encountered problem are high cardinality features, i.e.
unordered categorical predictor variables with a high number of levels. We
study techniques that yield numeric representations of categorical variables
which can then be used in subsequent ML applications. We focus on the impact of
those techniques on a subsequent algorithm's predictive performance, and -- if
possible -- derive best practices on when to use which technique. We conducted
a large-scale benchmark experiment, where we compared different encoding
strategies together with five ML algorithms (lasso, random forest, gradient
boosting, k-nearest neighbours, support vector machine) using datasets from
regression, binary- and multiclass- classification settings. Throughout our
study, regularized versions of target encoding (i.e. using target predictions
based on the feature levels in the training set as a new numerical feature)
consistently provided the best results. Traditional encodings that make
unreasonable assumptions to map levels to integers (e.g. integer encoding) or
to reduce the number of levels (possibly based on target information, e.g. leaf
encoding) before creating binary indicator variables (one-hot or dummy
encoding) were not as effective.},
year = {2022},
title = {Regularized target encoding outperforms traditional methods in supervised machine learning with high cardinality features},
journal = {Computational Statistics},
copyright = {http://creativecommons.org/licenses/by/4.0},
language = {eng},
author = {Pargent, F. and Pfisterer, F. and Thomas, J. and Bischl, B.},
}

@book{OhlssonJewell,
  title={Simplified estimation of structure parameters in hierarchical credibility},
  author = {Ohlsson, E.},
  year={2005},
  publisher={Mathematical Statistics, Stockholm University}
}

@book{JewellModel,
series = {International Institute for applied systems analysis. Research memoranda RM-75-24},
publisher = {IIASA},
year = {1975},
title = {The use of collateral data in credibility theory : a hierarchical model},
address = {Laxenburg},
author = {Jewell, W.S.},
keywords = {Statistics of stochastic processes. Estimation of stochastic processes. Hypothesis testing. Statistics of point processes. Time series analysis. Auto-correlation. Regression},
}

@book{Molenberghs2005,
series = {Springer Series in Statistics},
publisher = {Springer New York},
isbn = {9780387251448},
year = {2005},
title = {Models for Discrete Longitudinal Data},
language = {eng},
address = {New York, NY},
author = {Molenberghs, G. and Verbeke, G.},
keywords = {Statistics ; Statistical Theory and Methods ; Statistics for Life Sciences, Medicine, Health Sciences ; Statistics ; Mathematics},
}

@inproceedings{Hachemeister,
  title={Credibility for regression models with application to trend},
  author={Hachemeister, C.A. },
  booktitle={Credibility, theory and applications, Proceedings of the berkeley Actuarial Research Conference on Credibility},
  pages={129--163},
  year={1975}
}

@book{Dannenburg,
  title={Practical actuarial credibility models},
  author={Dannenburg, D. R. and Kaas, R.  and Goovaerts, M.J.},
  publisher = {IAE (Institute of Actuarial Science and Econometrics of the University of Amsterdam)},
  address = {Amsterdam},
  year={1996},
}

@book{Buhlmann2005,
abstract = {For practicing experts in the financial arena, in particular actuaries in the field of property - casualty insurance, life insurance, reinsurance and insurance supervision, and teachers and students. This book provides an exploration of Credibility Theory covering the aspects of this topic from the simplest case to the most detailed dynamic model.},
publisher = {Springer},
isbn = {3540257535},
year = {2006},
title = {Course in Credibility Theory and its Applications},
language = {eng},
address = {Berlin/Heidelberg},
author = {B{\"u}hlmann, H. and Gisler, A.},
keywords = {Credibility theory (Insurance) ; Econometrische modellen ; Grootste aannemelijkheid ; gtt ; Schattingstheorie},
}

@article{Wuthrich,
issn = {2190-9733},
abstract = {Generalized linear models have the important property of providing unbiased estimates on a portfolio level. This implies that generalized linear models manage to provide accurate prices on a portfolio level. On the other hand, neural networks may provide very accurate prices on an individual policy level, but state-of-the-art use of neural networks does not pay any attention to unbiasedness on the portfolio level. This is an implicit consequence of applying early stopping rules in gradient descent methods for model fitting. In the present paper we discuss this deficiency and we provide two different techniques to overcome this drawback of neural network model fitting.},
journal = {European actuarial journal},
pages = {179--202},
volume = {10},
publisher = {Springer Nature B.V},
number = {1},
year = {2020},
title = {Bias regularization in neural network models for general insurance pricing},
copyright = {EAJ Association 2019.},
language = {eng},
address = {Heidelberg},
author = {W{\"u}thrich, M.V.},
keywords = {Insurance rates ; Neural networks ; Bias ; Generalized linear models},
}
